---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---
# Setup
```{r}
library(tidyverse)
library(request)
```
# Data Import
## API requests
```{r}
baseurl = 'https://s5phub.copernicus.eu/dhus/odata/v1/Products'
count_path ='dhus/odata/v1/Products/$count'
query_path = 'dhus/odata/v1/Products/'

# S5P credentials 
username = ''
pwd = ''

# Create list of parameters to pass on to query later
parameter_list <- list(    
    `$filter` = "substringof('L2__NO2',Name) and year(ContentDate/End) eq 2020 and month(ContentDate/End) le 3"
    )
```
 
```{r}
# Find the number of results yielded by the API query 
count_resp <- GET(
  modify_url(baseurl, path = count_path), 
  query = parameter_list,
  authenticate(username, pwd)
  )
results_count <- content(count_resp)
results_count

# Create Function that takes in parameters for a GET request and returns the results of the API query
NO2_retreive <- function(skip_count = 0L, query_params = parameter_list) {
  query_params <- append(query_params,list(`$skip` = skip_count, `$format` = 'text/csv'))
  query_resp = GET(
    modify_url(baseurl, path = query_path), 
    authenticate(username, pwd),
    query = query_params
  )
  return(content(query_resp))
}

# Prepare base variables for loop
NO2_resp = NO2_retreive()
NO2_tbl = NO2_resp
counter = nrow(NO2_tbl)

# Retrieve all IDs that match filter by adjusting skipcounter until it's not equal to 50
while (nrow(NO2_resp) == 50) {
  
  NO2_resp = NO2_retreive(skip_count = counter)
  NO2_tbl <- bind_rows(
    NO2_tbl,
    NO2_resp
    )
  counter = counter + 50
  # Sys.sleep(0.5) #Being nice to server
  print(paste("Fetched",nrow(NO2_tbl),"results out of", results_count))
}
NO2_tbl
```

# Export Results
```{r}
NO2_output <- NO2_tbl %>% 
  mutate(downloadURL = paste0("https://s5phub.copernicus.eu/dhus/odata/v1/Products('",Id,"')/$value") ) %>% 
  filter(str_detect(Name,"_OFFL"))
  
NO2_output %>%
  write_csv("NO2_URL_list.csv")

# Read in cached list (from python) of files that can read
# Python is used curl and ensure that the curl results can read in properly. 
goodfiles = read_csv("../gfnames.csv",col_names = "fname") %>% 
  mutate(fname = str_replace(basename(fname)," .*",'')) 

# Get list of nc files to retreive
NO2_output %>% 
  anti_join(goodfiles, by = c("Name"="fname")) %>% 
  mutate(curl_cmd = paste0(
  "curl -u ", username,":", pwd," ", downloadURL," > ",Name)
  ) %>% View
```



